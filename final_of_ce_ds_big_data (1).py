# -*- coding: utf-8 -*-
"""Final of CE-DS Big_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1byq7REKIRoSqGbjaug0wrI5jc6MQx9t-
"""

import os
import numpy as np
import pandas as pd
import sklearn
import matplotlib.pylab as plt
import seaborn as sns
from scipy.stats import norm
from scipy.integrate import quad
from scipy.spatial import distance

from google.colab import files
uploaded = files.upload()

os.listdir()

data =  pd.read_csv('concrete.csv')
data_original = data.copy()

data.head()

data.describe()

"""Below shows how to view the cardinality and dimensionality size of the data"""

data.shape

# number 0 represent the rows and 1 is for columns
print('data has a cardinality size {}'.format(data.shape[0]) + 
      ' and dimensionality size {}'.format(data.shape[1]))

"""Checking the columns have the correct classification of data type. """

data.info()

"""## Data Cleaning
Before starting, we need to check the data for any missing values and outliers.
"""

data.isna().sum()

"""## As you can see, there is no NAN (not a number) in the columns.

"""

strength_max = data.strength.max()
strength_min = data.strength.min()
print("max:",strength_max)
print("min:",strength_min)

"""## Outliers
Using histograms and boxplots we can identify any ouliers in the data set.
"""

f, axes = plt.subplots(1, 2, figsize=(23, 6), sharex=True)
sns.distplot(data['slag'], ax = axes[0])
sns.boxplot(data['slag'], ax = axes[1])

"""As we can see, the values above 350 in the slag are outliers. To clean the data we should remove all records of data past this point.



"""

data[data['slag'] > 350]

"""Removing all entries that are above 350.

"""

data = data.drop(data[data['slag'] > 350].index)

f, axes = plt.subplots(1, 2, figsize=(23, 6), sharex=True)
sns.distplot(data['slag'], ax = axes[0])
sns.boxplot(data['slag'], ax = axes[1])

"""And now the outliers in water"""

f, axes = plt.subplots(1, 2, figsize=(23, 6), sharex=True)
sns.distplot(data['water'], ax = axes[0])
sns.boxplot(data['water'], ax = axes[1])

"""Here the outliers are before 122 and after 230."""

data[(data['water'] < 122) | (data['water'] > 230)]

"""So once again, we drop those values from the column."""

data = data.drop(data[(data['water'] < 122) | (data['water'] > 230)].index)

"""Further examples of cleaning; superplastic."""

f, axes = plt.subplots(1, 2, figsize=(23, 6), sharex=True)
sns.distplot(data['superplastic'], ax = axes[0])
sns.boxplot(data['superplastic'], ax = axes[1])

data = data.drop(data[data['superplastic'] > 25].index)
data[data['superplastic'] > 25]

"""cleaning fineagg"""

f, axes = plt.subplots(1, 2, figsize=(23, 6), sharex=True)
sns.distplot(data['fineagg'], ax = axes[0])
sns.boxplot(data['fineagg'], ax = axes[1])

data[(data['fineagg'] < 600) | (data['fineagg'] > 950)]

data = data.drop(data[(data['fineagg'] < 600) | (data['fineagg'] > 950)].index)

"""Cleaning age"""

f, axes = plt.subplots(1, 2, figsize=(23, 6), sharex=True)
sns.distplot(data['age'], ax = axes[0])
sns.boxplot(data['age'], ax = axes[1])

data[data['age'] > 150]

data = data.drop(data[data['age'] > 150].index)

"""# Binning
Binning alows us to seperate our data into "bins", that are created with ranges we set. The binning that happens below divides the data into two catagories.

The first is all concrete samples that we are going to class as standard (0-50 strength). The second (50- max strength) are the samples we will class as strong.
"""

strength_max = data.strength.max()
strength_min = data.strength.min()
print("max:",strength_max)
print("min:",strength_min)

strengthM = data['strength'].mean()

data['strength_cat'] = pd.cut(data['strength']
                              , bins=[0,strengthM,strength_max]
                              , labels=[0,1])
data["strength_cat"].hist()
print(data["strength_cat"].value_counts())

data.head()

# class count
count_class_standard, count_class_strong = data.strength_cat.value_counts()
#Divide by class
data_class_standard = data[data['strength_cat']== 0]
data_class_strong = data[data['strength_cat']== 1]

"""## **Under-sampling**
The way it work is by randomly deleting samples from the major class until we reach a balance between the 2 classes.

The problem with this method is that we will lose some random samples of our data that could be useful or even critical for our dataset as it will select the samples randomly without being able to tell how useful the deleted data was otherwise.

So our data should be larg enough to handle the under-sampling
"""

data_class_standard_under = data_class_standard.sample(count_class_strong)
data_under = pd.concat([data_class_standard_under, data_class_strong], axis=0)

print('Random under-sampling:')
print(data_under.strength_cat.value_counts())

data_under.strength_cat.value_counts().plot(kind='bar', title='Count (target)');

"""

## **Over**-**sampling**
Random over sampling will randomly create clone samples from the minority class and add them to the minority class, which will give them a higher chance to be selected.

oversampling minor class with a sever high balance can cause an overfitting for that class resulting in better performance on the training dataset but it will fail on the test dataset.

Another disadvantage of oversampling is that it increases the number of training samples which will result in increasing the training time."""

data_class_strong_over = data_class_strong.sample(count_class_standard, replace=True)
data_test_over = pd.concat([data_class_standard, data_class_strong_over], axis=0)

print('Random over-sampling:')
print(data_test_over.strength_cat.value_counts())

data_test_over.strength_cat.value_counts().plot(kind='bar', title='Count (strength)');

"""## Quantile binning
Using quartiles to visualise the data.
The red lines outline the quartile values, to help with deciding on bins.
"""

quantile_list = [0, .25, .5, .75, 1.]
quantiles = data['strength'].quantile(quantile_list)
quantiles

'ADAPTIVE BINNING : 4-quantile'
fig, ax = plt.subplots()
data['strength'].hist(bins=10, color='#abd3a9', 
                             edgecolor='black', grid=False)
for quantile in quantiles:
    qvl = plt.axvline(quantile, color='r')
ax.legend([qvl], ['Quantiles'], fontsize=10)
ax.set_title('strength with Quantiles', 
             fontsize=12)
ax.set_xlabel('strength', fontsize=12)
ax.set_ylabel('frequency', fontsize=12)

"""## Heatmaps
Heatmaps can also be created. These help with visualizing patterns, and potential corralations within the data set.
https://seaborn.pydata.org/generated/seaborn.heatmap.html
"""

plt.figure(figsize=(23,5))
sns.heatmap(data=data.corr(), annot=True);

corr = data.corr()
corr.style.background_gradient(cmap='coolwarm')

"""## PCA"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# initializing standard scaler, to be used for data scaling
scaler = StandardScaler()

drop_data = data.copy()
output_data = drop_data.loc[:, ['strength_cat']]
drop_data = drop_data.drop(columns=['strength_cat'])

# Scaling PCA
drop_data_scaled = drop_data.copy()
drop_data_scaled[drop_data_scaled.columns] = scaler.fit_transform(drop_data_scaled)

# n=1 as we only want to select one component
pca = PCA(n_components=1)

#selecting 3 attributes to get a combination
a_dict = dict()   # dictionary to hold the variance of group of attributes        dict[attributes-name] = 1st PCA comp

# Three attribute group
for i in range (len(drop_data_scaled.columns) - 2):
    for j in range (i+1, len(drop_data_scaled.columns) - 1):
        for k in range (j+1, len(drop_data_scaled.columns) - 0):
            input_cols = [drop_data_scaled.columns[i], drop_data_scaled.columns[j], drop_data_scaled.columns[k]]
            col_str = "" + drop_data_scaled.columns[i] + "," + drop_data_scaled.columns[j] + "," + drop_data_scaled.columns[k]
            pca_strength = pca.fit_transform(drop_data_scaled[input_cols]) # applying PCA
            var = np.round(pca.explained_variance_ratio_, decimals=3) * 100 # variance on the first component
            print(col_str,  "=" , var)
            a_dict[col_str] = var

import operator
# sorting the dictionary
sorted_dict = {k: v for k, v in sorted(a_dict.items(), key=lambda item: item[1])}
single_a_dict = dict()

count = 0

# using the first 35
for key in sorted_dict:  
    temp = key.split(",")  # key split to see attributes
    for val in temp:       # for each attribute, count how many times it is present in the ammount specified
        if val in single_a_dict:
            single_a_dict[val] += 1
        else:
            single_a_dict[val] = 1
    if count == 35:
        break
    count+=1

#Using bargraph to visualise

df = pd.DataFrame([single_a_dict])

plt.xticks(rotation='vertical')
plt.bar(single_a_dict.keys(), single_a_dict.values(), width=0.5, color='g')

df.head()

data_items = single_a_dict.items()
data_list = list(data_items)

df = pd.DataFrame(data_list)

df.head()

df.columns=['key','value']

value_cut_off_point = 12.5

df.loc[df['value'] <= value_cut_off_point, 'key']

PCA_remove_count = len(df.loc[df['value'] <= value_cut_off_point, 'key'])
print(PCA_remove_count)

data_under.head()

i = 0
while i < PCA_remove_count:
  x = df.loc[df['value'] <= value_cut_off_point, 'key'].iloc[i]
  data = data.drop(columns=[x])
  i = i + 1

data.head()

from imblearn.over_sampling import SMOTE
oversample = SMOTE()

X = data.drop(['strength_cat'], axis=1)
y = data.strength_cat

features_X, target_y = oversample.fit_resample(X, y)

print('SMOTE over sampling:')
print(target_y.value_counts())
target_y.value_counts().plot(kind='bar', title='Count (target)');

X.head()

y.head()

import statsmodels.api as stat
from statsmodels.stats.outliers_influence import variance_inflation_factor 
data_final = data.copy()


#VIF calculation
VIFcalc = stat.tools.add_constant(data_final)
S = pd.Series([variance_inflation_factor(VIFcalc.values, i) for i in range(VIFcalc.shape[1])], index=VIFcalc.columns)
print('Output: \n\n{}\n'.format(S))

#print('Recomended removal:')
#for x in S.index:
#  if S.loc[x]> 6:
#    print(str(x))
#    if x != 'const':
 #     data_final = data_final.drop(columns=[str(x)])

data_final.info()

"""# Spliting the Data into test and train sets"""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_roc_curve
from sklearn.model_selection import StratifiedKFold
import pandas as pd

data = data.copy()

y = pd.DataFrame(data.strength_cat)
X = pd.DataFrame(data, columns=data.columns)
X = X.drop(columns=['strength_cat'])

X.head()

y.head()

X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    train_size=0.8, 
                                                    random_state=1,
                                                    stratify=y)

print('X_train shape', y_train.shape, 'X_test shape', y_test.shape)

print('train_class_one_ratio', np.count_nonzero(y_train==1) / len(y_train))
print('test_class_one_ratio', np.count_nonzero(y_test==1) / len(y_test))

"""# Stratified k-fold cross validation, to estimate model accuracy"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier

def stacking():
  # Including the different models to be used for stacking, base level L0
	L0 = []
	L0.append(('KNN', KNeighborsClassifier()))
	L0.append(('CART', DecisionTreeClassifier()))
	L0.append(('NB', GaussianNB()))
	L0.append(('SVM', SVC(gamma='auto')))
	L0.append(('LR', LogisticRegression(max_iter=10000)))
	L0.append(('SGDClassifier', SGDClassifier(loss="hinge", penalty="l2", max_iter=10000)))
	L0.append(('RandomForestClassifier', RandomForestClassifier(n_estimators=10)))
	L0.append(('AdaBoostClassifier', AdaBoostClassifier(n_estimators=100)))
	# Define meta learner model as L1
	L1 = LogisticRegression(max_iter=10000)
	# Define the stacking ensemble
	model = StackingClassifier(estimators=L0, final_estimator=L1, cv=10)
	return model

# Spot Check Algorithms for class_label classification
pred_models = []
pred_models.append(('KNN', KNeighborsClassifier()))
pred_models.append(('DecisionTreeClassifier', DecisionTreeClassifier()))
pred_models.append(('GaussianNB', GaussianNB()))
pred_models.append(('SupportVectorMachine', SVC(gamma='auto')))
pred_models.append(('LogisticRegression', LogisticRegression(max_iter=10000)))
pred_models.append(('SGDClassifier', SGDClassifier(loss="hinge", penalty="l2", max_iter=10000)))
pred_models.append(('RandomForestClassifier', RandomForestClassifier(n_estimators=10)))
pred_models.append(('AdaBoostClassifier', AdaBoostClassifier(n_estimators=100)))
pred_models.append(('RandomForestClassifierOPTIMISED', RandomForestClassifier(bootstrap=True, max_depth=110, max_features= 'sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=1000)))

#Stacking model 
pred_models.append(('Stacking', stacking()))


# evaluate each model, one after another.Uses 5-fold cross validation
results = []
names = []
for name, model in pred_models:
	skf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)
	cross_val_results = cross_val_score(model, X_train, y_train.values.ravel(), cv=skf, scoring='accuracy')
	results.append(cross_val_results)
	names.append(name)
	print('%s: %f (%f)' % (name, cross_val_results.mean(), cross_val_results.std()))




# Comparison of the different models:

stack = stacking()
stack.fit(X_train, y_train)
y_pred = stack.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm2, annot=True, fmt=".0f")

print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
stack_roc_auc = roc_auc_score(y_test, stack.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, stack.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Stacking method (area = %0.2f)' % stack_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

RFO = RandomForestClassifier(bootstrap=True, max_depth=110, max_features= 'sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=1000)
RFO.fit(X_train, y_train)
y_pred = RFO.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm2, annot=True, fmt=".0f")

print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
RFO_roc_auc = roc_auc_score(y_test, RFO.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, RFO.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Optimised Random Forest method (area = %0.2f)' % RFO_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred = nb.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm2, annot=True, fmt=".0f")

print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
nb_roc_auc = roc_auc_score(y_test, nb.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, nb.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Naive Bayes (area = %0.2f)' % nb_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

from sklearn import tree
dt = tree.DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
cm1 = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm1, annot=True, fmt=".0f")

print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
dt_roc_auc = roc_auc_score(y_test, dt.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, dt.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Decision tree (area = %0.2f)' % dt_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm2, annot=True, fmt=".0f")

print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
knn_roc_auc = roc_auc_score(y_test, knn.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, knn.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='K-nearest Neighbor (area = %0.2f)' % knn_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

SVM = SVC(gamma='auto')
SVM.fit(X_train, y_train)
y_pred = SVM.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm2, annot=True, fmt=".0f")

print(classification_report(y_test, y_pred))

LOGR = LogisticRegression()
LOGR.fit(X_train, y_train)
y_pred = nb.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm2, annot=True, fmt=".0f")

print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
LOGR_roc_auc = roc_auc_score(y_test, LOGR.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, LOGR.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistical Regression (area = %0.2f)' % LOGR_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

SGD = SGDClassifier(loss="hinge", penalty="l2", max_iter=10000)
SGD.fit(X_train, y_train)
y_pred = SGD.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm2, annot=True, fmt=".0f")

print(classification_report(y_test, y_pred))

ADA = AdaBoostClassifier(n_estimators=100)
ADA.fit(X_train, y_train)
y_pred = ADA.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm2, annot=True, fmt=".0f")

print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
ADA_roc_auc = roc_auc_score(y_test, ADA.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, ADA.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='AdaBoost Classifier (area = %0.2f)' % ADA_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

RF = RandomForestClassifier(n_estimators=10)
RF.fit(X_train, y_train)
y_pred = RF.predict(X_test)
cm2 = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm2, annot=True, fmt=".0f")

print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
RF_roc_auc = roc_auc_score(y_test, RF.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, RF.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Random Forest (area = %0.2f)' % RF_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()